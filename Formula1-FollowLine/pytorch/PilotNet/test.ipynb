{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/miniconda3/envs/dlstudio2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from utils.processing import *\n",
    "from utils.pilot_net_dataset import PilotNetDataset\n",
    "from utils.pilotnet import PilotNet\n",
    "from utils.transform_helper import createTransform\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# Device Selection (CPU/GPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\") # support available only for cpu\n",
    "cast = torch.Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = './experiments/optimized_models/trained_models'\n",
    "qmodel_path = model_save_dir + '/static_quan.ckpt'\n",
    "quant_model = torch.jit.load(qmodel_path)\n",
    "# print(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quant_model._weight_bias()\n",
    "# for p in quant_model.named_children():\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PilotNet(\n",
      "  (ln_1): BatchNorm2d(3, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cn_1): Conv2d(3, 24, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (cn_2): Conv2d(24, 36, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (cn_3): Conv2d(36, 48, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (cn_4): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (cn_5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc_1): Linear(in_features=1152, out_features=1164, bias=True)\n",
      "  (fc_2): Linear(in_features=1164, out_features=100, bias=True)\n",
      "  (fc_3): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc_4): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc_5): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pilotModel = PilotNet([200, 66, 3], 2).to(device)\n",
    "# pilotModel.load_state_dict(torch.load('experiments/retrain_best/trained_models/pilot_net_model_121.ckpt'))\n",
    "print(pilotModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in m.children():\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/miniconda3/envs/dlstudio2/lib/python3.10/site-packages/torch/ao/quantization/observer.py:177: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0080]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (1): QuantizedBatchNorm2d(3, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): QuantizedConv2d(3, 24, kernel_size=(5, 5), stride=(2, 2), scale=0.019769079983234406, zero_point=64)\n",
       "  (3): QuantizedConv2d(24, 36, kernel_size=(5, 5), stride=(2, 2), scale=0.011379468254745007, zero_point=66)\n",
       "  (4): QuantizedConv2d(36, 48, kernel_size=(5, 5), stride=(2, 2), scale=0.007137960754334927, zero_point=62)\n",
       "  (5): QuantizedConv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.004424086306244135, zero_point=54)\n",
       "  (6): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0025282984133809805, zero_point=62)\n",
       "  (7): Flatten(start_dim=1, end_dim=-1)\n",
       "  (8): QuantizedLinear(in_features=1152, out_features=1164, scale=0.0019415776478126645, zero_point=61, qscheme=torch.per_channel_affine)\n",
       "  (9): QuantizedLinear(in_features=1164, out_features=100, scale=0.0010156267089769244, zero_point=69, qscheme=torch.per_channel_affine)\n",
       "  (10): QuantizedLinear(in_features=100, out_features=50, scale=0.0017847600392997265, zero_point=63, qscheme=torch.per_channel_affine)\n",
       "  (11): QuantizedLinear(in_features=50, out_features=10, scale=0.001652304781600833, zero_point=55, qscheme=torch.per_channel_affine)\n",
       "  (12): QuantizedLinear(in_features=10, out_features=2, scale=0.001577527727931738, zero_point=5, qscheme=torch.per_channel_affine)\n",
       "  (13): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = deepcopy(pilotModel)\n",
    "m.eval()\n",
    "backend = \"fbgemm\"\n",
    "\"\"\"Insert stubs\"\"\"\n",
    "m = nn.Sequential(torch.quantization.QuantStub(), \n",
    "                  *nn.Sequential(*(m.children())), \n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "\"\"\"Prepare\"\"\"\n",
    "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.quantization.prepare(m, inplace=True)\n",
    "\n",
    "\"\"\"Calibrate\n",
    "- This example uses random data for convenience. Use representative (validation) data instead.\n",
    "\"\"\"\n",
    "with torch.inference_mode():\n",
    "  for _ in range(10):\n",
    "    x = torch.rand(1,3, 200, 66)\n",
    "    m(x)\n",
    "    \n",
    "\"\"\"Convert\"\"\"\n",
    "torch.quantization.convert(m, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1.weight', Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True))\n",
      "('1.bias', Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# m.__dict__ \n",
    "# print(m[0][[1]].weight().element_size())\n",
    "\n",
    "for p in m.named_parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m._modules['1'].ln_1.weight.element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(1,3,200,66)\n",
    "\n",
    "m.eval()\n",
    "with torch.no_grad():\n",
    "    out = m(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dlstudio2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a593078fe06cb7ae73e3b1c0a557fb9e1d7fe54786371c2d6019d7a5d43fc4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
