I"eg<p>First of all you need to install Deep Learning Studio. If you haven’t completed that step, please go to the <a href="/install/">installation section</a>.</p>

<p>We additionally have some implemented algorithms that you can use in Deep Learning Studio. Find them in the <a href="/quick_start/algorithms_zoo/">algorithms zoo</a>.</p>

<p>If you’d like to train your own brain, we provide you with the <a href="/quick_start/datasets">datasets</a>.</p>

<p>This repository contains the deep learning regression and classification models for all robots used in the JdeRobot community.</p>

<h2 id="structure-of-the-branch">Structure of the branch</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── Formula1-FollowLine
|   |
|   |── pytorch
|   |   |── PilotNet                                # Pilot Net pytorch implementation
|   |   |   ├── scripts                             # scripts for running experiments 
|   |   |   ├── utils                               
|   |   |   |   ├── pilot_net_dataset.py            # Torchvision custom dataset
|   |   |   |   ├── pilotnet.py                     # CNN for PilotNet
|   |   |   |   ├── transform_helpers.py            # Data Augmentation
|   |   |   |   └── processing.py                   # Data collecting, processing and utilities
|   |   |   └── train.py                            # training code
|   |   |
|   |   └── PilotNetStacked                         # Pilot Net Stacked Image implementation
|   |       ├── scripts                             # scripts for running experiments 
|   |       ├── utils                               
|   |       |   ├── pilot_net_dataset.py            # Sequentially stacked image dataset
|   |       |   ├── pilotnet.py                     # Modified Hyperparams 
|   |       |   ├── transform_helpers.py            # Data Augmentation
|   |       |   └── processing.py                   # Data collecting, processing and utilities
|   |       └── train.py                            # training code
|   |
|   ├── tensoflow
|       |── PilotNet                                # Pilot Net tensorflow implementation
|           ├── utils                               
|           |   ├── dataset.py                      # Custom dataset
|           |   ├── pilotnet.py                     # CNN for PilotNet
|           |   └── processing.py                   # Data collecting, processing and utilities
|           └── train.py                            # training code
├── Drone-FollowLine
    |
    |── DeepPilot                               # DeepPilot CNN pytorch implementation
    |   ├── scripts                             # scripts for running experiments 
    |   ├── utils                               
    |   |   ├── pilot_net_dataset.py            # Torchvision custom dataset
    |   |   ├── pilotnet.py                     # CNN for DeepPilot
    |   |   ├── transform_helpers.py            # Data Augmentation
    |   |   └── processing.py                   # Data collecting, processing and utilities
    |   └── train.py                            # training code
</code></pre></div></div>

<h2 id="how-to-use">How to use</h2>

<p>First of all <strong>if you are going to use the GUI</strong> you need to create the resources file for the application. <strong>YOU JUST HAVE TO DO THIS STEP THE FIRST TIME YOU RUN THE APPLICATION</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyrcc5 <span class="nt">-o</span> ui/gui/resources/resources.py ui/gui/resources/resources.qrc
</code></pre></div></div>

<p>To launch the application just run the python script as follows:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python driver.py <span class="nt">-c</span> ./configs/default.yml <span class="nt">-g</span>
</code></pre></div></div>

<p>This command will launch the application with the default configuration:</p>

<ul>
  <li>Simulated world of a F1 car inside a circuit</li>
  <li>Sensors: RGB camera and odometry</li>
  <li>Actuators: Motors</li>
  <li>Explicit brain based on OpenCV image processing.</li>
</ul>

<p>The program allows the following arguments:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">-c &lt;profile&gt;</code> or <code class="language-plaintext highlighter-rouge">--config &lt;profile&gt;</code>: this argument is mandatory and specifies the path of the configuration file for the application.</li>
  <li><code class="language-plaintext highlighter-rouge">-g</code> or <code class="language-plaintext highlighter-rouge">--gui</code>: this argument is optional and enables the GUI launching. If not specified, it will show the TUI (terminal user interface).</li>
  <li><code class="language-plaintext highlighter-rouge">-t</code> or <code class="language-plaintext highlighter-rouge">--tui</code>: this argument is optional and enables the TUI.</li>
  <li><code class="language-plaintext highlighter-rouge">-s</code> or <code class="language-plaintext highlighter-rouge">--script</code>: this argument is optional and enables the scriptable application.</li>
  <li><code class="language-plaintext highlighter-rouge">-r</code> or <code class="language-plaintext highlighter-rouge">--random</code>: this argument is optional and enables initialization of the Formula-1 car at random positions on the circuit with random orientation.</li>
</ul>

<p>For more information run <code class="language-plaintext highlighter-rouge">help driver.py</code> in a terminal.</p>

<h3 id="building-your-configuration-file">Building your configuration file</h3>

<p>If you want to create your own <strong>configuration file</strong> for the application (changing the robot, brain, layout, etc) you can either use the desktop GUI or creating a yml file with your own configuration. The default profile looks like this (<strong>Make sure you respect the indentation</strong>):</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">Behaviors</span><span class="pi">:</span>
    <span class="na">Robot</span><span class="pi">:</span>
        <span class="na">Sensors</span><span class="pi">:</span>
            <span class="na">Cameras</span><span class="pi">:</span>
                <span class="na">Camera_0</span><span class="pi">:</span>
                    <span class="na">Name</span><span class="pi">:</span> <span class="s1">'</span><span class="s">camera_0'</span>
                    <span class="na">Topic</span><span class="pi">:</span> <span class="s1">'</span><span class="s">/F1ROS/cameraL/image_raw'</span>
            <span class="na">Pose3D</span><span class="pi">:</span>
                <span class="na">Pose3D_0</span><span class="pi">:</span>
                    <span class="na">Name</span><span class="pi">:</span> <span class="s1">'</span><span class="s">pose3d_0'</span>
                    <span class="na">Topic</span><span class="pi">:</span> <span class="s1">'</span><span class="s">/F1ROS/odom'</span>
        <span class="na">Actuators</span><span class="pi">:</span>
            <span class="na">Motors</span><span class="pi">:</span>
                <span class="na">Motors_0</span><span class="pi">:</span>
                    <span class="na">Name</span><span class="pi">:</span> <span class="s1">'</span><span class="s">motors_0'</span>
                    <span class="na">Topic</span><span class="pi">:</span> <span class="s1">'</span><span class="s">/F1ROS/cmd_vel'</span>
                    <span class="na">MaxV</span><span class="pi">:</span> <span class="m">3</span>
                    <span class="na">MaxW</span><span class="pi">:</span> <span class="m">0.3</span>
        <span class="na">BrainPath</span><span class="pi">:</span> <span class="s1">'</span><span class="s">brains/f1/brain_f1_opencv2.py'</span>
        <span class="na">Type</span><span class="pi">:</span> <span class="s1">'</span><span class="s">f1'</span>
    <span class="na">Simulation</span><span class="pi">:</span>
        <span class="na">World</span><span class="pi">:</span> <span class="s">/opt/jderobot/share/jderobot/gazebo/launch/f1_1_simplecircuit.launch</span>
    <span class="na">Dataset</span><span class="pi">:</span>
        <span class="na">In</span><span class="pi">:</span> <span class="s1">'</span><span class="s">/tmp/my_bag.bag'</span>
        <span class="na">Out</span><span class="pi">:</span> <span class="s1">'</span><span class="s">'</span>
    <span class="na">Layout</span><span class="pi">:</span>
        <span class="na">Frame_0</span><span class="pi">:</span>
            <span class="na">Name</span><span class="pi">:</span> <span class="s">frame_0</span>
            <span class="na">Geometry</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">1</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">,</span> <span class="nv">2</span><span class="pi">,</span> <span class="nv">2</span><span class="pi">]</span>
            <span class="na">Data</span><span class="pi">:</span> <span class="s">rgbimage</span>
        <span class="na">Frame_1</span><span class="pi">:</span>
            <span class="na">Name</span><span class="pi">:</span> <span class="s">frame_1</span>
            <span class="na">Geometry</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">0</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">]</span>
            <span class="na">Data</span><span class="pi">:</span> <span class="s">rgbimage</span>
        <span class="na">Frame_2</span><span class="pi">:</span>
            <span class="na">Name</span><span class="pi">:</span> <span class="s">frame_2</span>
            <span class="na">Geometry</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">0</span><span class="pi">,</span> <span class="nv">2</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">]</span>
            <span class="na">Data</span><span class="pi">:</span> <span class="s">rgbimage</span>
        <span class="na">Frame_3</span><span class="pi">:</span>
            <span class="na">Name</span><span class="pi">:</span> <span class="s">frame_3</span>
            <span class="na">Geometry</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">0</span><span class="pi">,</span> <span class="nv">3</span><span class="pi">,</span> <span class="nv">3</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">]</span>
            <span class="na">Data</span><span class="pi">:</span> <span class="s">rgbimage</span>
</code></pre></div></div>

<p>The keys of this file are as follows:</p>

<p><strong>Robot</strong></p>

<p>This key defines the robot configuration:</p>

<ul>
  <li><strong>Sensors</strong>: defines the sensors configuration. Every sensor <strong>must</strong> have a name and a ROS topic.</li>
  <li><strong>Actuators</strong>: defines the actuators configuration. Every actuator <strong>must</strong> have a name and a ROS topic.</li>
  <li><strong>BrainPath</strong>: defines the path where the control algorithm is located in your system.</li>
  <li><strong>Type</strong>: defines the type of robot. Possible values: f1, drone, turtlebot, car.</li>
</ul>

<p><strong>Simulation</strong></p>

<p>This key defines the launch file of the environment. It can be used to launch a gazebo simulation or a real robot.</p>

<p><strong>Dataset</strong></p>

<p>This key define the dataset output path.</p>

<p><strong>Layout</strong></p>

<p>This key defines how the GUI will be shown. This is the trickiest part of the configuration file. The grid of the GUI is a 3x3 matrix where you can configure the layout positions to show your sensors data. Each <em>Frame_X</em> key corresponds to a view of a sensor in the GUI and includes the following keys:</p>

<ul>
  <li>Name: this is mandatory in order to send data to that frame in the GUI to be shown.</li>
  <li>Geometry: is the position and size configuration of that frame following this criteria: ` [x, y, height, width]`</li>
  <li>Data: tells the GUI which kind of GUI should be create in order to show the information. Possible values: rgbimage, laser, pose3d.</li>
</ul>

<p>The geometry is defined as follows.</p>

<figure class=" ">
  
    
      <a href="/assets/images/matrix_schema.png">
        <img src="/assets/images/matrix_schema.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>So if you want to create a view for one of the camera sensors of the robot located in the top-left corner of size 1x1 followed by another view of another camera in the bottom-right corner of size 2x2, you should configure the geometry array as:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">Frame_0</span><span class="pi">:</span>
    <span class="na">Name</span><span class="pi">:</span> <span class="s1">'</span><span class="s">Camera1'</span>
    <span class="na">Geometry</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">0</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">,</span> <span class="nv">1</span><span class="pi">]</span>
    <span class="na">Data</span><span class="pi">:</span> <span class="s">rgbimage</span>
<span class="na">Frame_1</span><span class="pi">:</span>
    <span class="na">Name</span><span class="pi">:</span> <span class="s1">'</span><span class="s">Camera2'</span>
    <span class="na">Geometry</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">1</span><span class="pi">,</span> <span class="nv">2</span><span class="pi">,</span> <span class="nv">2</span><span class="pi">,</span> <span class="nv">2</span><span class="pi">]</span>
</code></pre></div></div>

<p>So it will look like this in the GUI:</p>

<figure class=" ">
  
    
      <a href="/assets/images/config1.png">
        <img src="/assets/images/config1.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>Following this logic, you will see that the <strong>default configuration</strong> file will show something like this:</p>

<figure class=" ">
  
    
      <a href="/assets/images/default_config.png">
        <img src="/assets/images/default_config.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p><strong>Custom Brain parameters</strong></p>

<p>Some parameters for custom brains can be set up directly from the <em>yml</em> file below the type of the robot which in this example is <em>f1</em>. This format is used to setup paths for trained models and any brain kwargs required by the user.</p>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">BrainPath</span><span class="pi">:</span> <span class="s1">'</span><span class="s">brains/f1/brain_f1_torch.py'</span>
<span class="na">Type</span><span class="pi">:</span> <span class="s1">'</span><span class="s">f1'</span>
<span class="na">Parameters</span><span class="pi">:</span>
    <span class="na">Model</span><span class="pi">:</span> <span class="s1">'</span><span class="s">trained_model_name.checkpoint'</span>
    <span class="na">ImageCrop</span><span class="pi">:</span> <span class="s">True</span> 
    <span class="na">ParamExtra</span><span class="pi">:</span> <span class="pi">{</span><span class="nv">Specify value</span><span class="pi">}</span>
</code></pre></div></div>

<p><strong>Reinforcement Learning parameters</strong></p>

<p>Optionally some parameters for reinforcement learning can be set up directly from the <em>yml</em> file below the type of the robot which in this example is <em>f1rl</em>.</p>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">BrainPath</span><span class="pi">:</span> <span class="s1">'</span><span class="s">brains/f1rl/train.py'</span>
<span class="na">Type</span><span class="pi">:</span> <span class="s1">'</span><span class="s">f1rl'</span>
<span class="na">Parameters</span><span class="pi">:</span>
    <span class="na">action_set</span><span class="pi">:</span> <span class="s1">'</span><span class="s">simple'</span>
    <span class="na">gazebo_positions_set</span><span class="pi">:</span> <span class="s1">'</span><span class="s">pista_simple'</span>
    <span class="na">alpha</span><span class="pi">:</span> <span class="s">0.2</span> 
    <span class="na">gamma</span><span class="pi">:</span> <span class="m">0.9</span>
    <span class="na">epsilon</span><span class="pi">:</span> <span class="m">0.99</span>
    <span class="na">total_episodes</span><span class="pi">:</span> <span class="m">20000</span>
    <span class="na">epsilon_discount</span><span class="pi">:</span> <span class="s">0.9986</span> 
    <span class="na">env</span><span class="pi">:</span> <span class="s1">'</span><span class="s">camera'</span>
</code></pre></div></div>

<h3 id="using-the-application">Using the application</h3>

<p>Once the configuration file is created and the application has launched, you will see something like this (depending on your layout configuration. We assume you launched the default profile):</p>

<figure class=" ">
  
    
      <a href="/assets/images/main_window.png">
        <img src="/assets/images/main_window.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>You will see 2 different sections, one on the left: <strong>the toolbar</strong>, and another one in the right: <strong>the layout</strong>.</p>

<h4 id="the-toolbar">The toolbar</h4>

<p>You have all the tools needed for controlling the whole application, simulation and control of the application. For usability sake, this section is subdivided in 4 different subsections: <strong>stats, dataset, brains</strong> and <strong>simulation</strong>.</p>

<figure class=" ">
  
    
      <a href="/assets/images/toolbar.png">
        <img src="/assets/images/toolbar.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p><strong>Stats</strong></p>

<figure class=" ">
  
    
      <a href="/assets/images/stats.png">
        <img src="/assets/images/stats.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>You can save metrics from the brain with the stats functionality. For saving, press play while the brain is running and 
press again to finish. After that, a general view of the stats should appear. For further detail, run the <code class="language-plaintext highlighter-rouge">show_plots.py</code> script:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    python3 show_plots.py -b [bag_name].bag -b [bag_name].bag
</code></pre></div></div>

<p>This script will load further information related with the execution.</p>

<p><strong>Visualizing Brain Performances</strong></p>

<p>The <code class="language-plaintext highlighter-rouge">behavior_metrics/show_plots.py</code> file uses the QtWindow to generate it runtime on the DISPLAY. So, the new script setup as found in <code class="language-plaintext highlighter-rouge">behavior_metrics/scripts/analyze_brain.bash</code> eases the overall process by first collecting the ROS bags with the confg file provided and then generates all the analysis plots. The argument for the analysis is the config file suitable for using with the <code class="language-plaintext highlighter-rouge">script</code> mode.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    bash scripts/analyze_brain.bash ./configs/default-multiple.yml
</code></pre></div></div>

<p>Finally, this saves everything at <code class="language-plaintext highlighter-rouge">behavior_metrics/bag_analysis/bag_analysis_plots/</code> directory sorted according to the different circuits. The current formulation of the saving plots analysis creates the following directory structure:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>behavior_metrics/bag_analysis
	+-- bag_analysis_plots/
	|	+-- circuit_name/ 						
	|   	+-- performances   
	|   	|   +-- completed_distance.png
	|   	|   +-- percentage_completed.png
	|   	|   +-- lap_seconds.png	
	|   	|   +-- circuit_diameter.png 		
	|			|   +-- average_speed.png 
	|   	+-- first_images/			
	|		  +-- path_followed/ 					
	+-- bags/ 
</code></pre></div></div>

<p><strong>Dataset</strong></p>

<figure class=" ">
  
    
      <a href="/assets/images/dataset.png">
        <img src="/assets/images/dataset.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>This subsection will allow you to specify where the datasets will be saved by setting up the path and the name of the bag file.</p>

<p>To specify the output ROS bag file, just click on the three dots and a dialog window will open with the file system.</p>

<p>The button <strong>Select topics</strong> is used to select which active topics the user wants to record in the ROS bag file.</p>

<p>Use the play button to start/stop recording the rosbag.</p>

<p><strong>Note: if you don’t change your ROS bag name between recordings, the file will be overwritten with the new recording data.</strong></p>

<p><strong>Brain</strong></p>

<figure class=" ">
  
    
      <a href="/assets/images/brain.png">
        <img src="/assets/images/brain.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>This subsection will allow you to control the logic of the robot: its behavior.</p>

<p>You have a drop-down menu that will detect the available brains for the current simulation, so you can select whatever brain you want in each moment. <strong>This can be done on the go, if the simulation is paused</strong></p>

<p>The <strong>Load</strong> button will load a new brain in execution time <strong>if the simulation is paused</strong></p>

<figure class=" ">
  
    
      <a href="/assets/images/change_brain.gif">
        <img src="/assets/images/change_brain.gif" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>All of this tools will be disabled while the simulation is running, so in order to interact with it, you should pause the simulation first.</p>

<p><strong>Simulation</strong></p>

<figure class=" ">
  
    
      <a href="/assets/images/simulation.png">
        <img src="/assets/images/simulation.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>This subsection will allow you to control the simulation.</p>

<p>You have a drop-down menu to change the world of the simulation on the go <strong>if the simulation is paused</strong></p>

<p>The <strong>Load</strong> button will load the specified world in the drop-down menu <strong>if the simulation is paused</strong></p>

<p>You have 3 additional buttons which:</p>

<ul>
  <li>Will load Gazebo GUI if it wasn’t launched, or close it otherwise</li>
</ul>

<figure class=" ">
  
    
      <a href="/assets/images/gzclient.gif">
        <img src="/assets/images/gzclient.gif" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<ul>
  <li>Play/pause button for <strong>pausing/resuming the simulation</strong></li>
</ul>

<figure class=" ">
  
    
      <a href="/assets/images/brain_sim.gif">
        <img src="/assets/images/brain_sim.gif" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<ul>
  <li>Reload button will reload the simulation by resetting the robot position and both real and simulation time.</li>
</ul>

<figure class=" ">
  
    
      <a href="/assets/images/reload_sim.gif">
        <img src="/assets/images/reload_sim.gif" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>respectively.</p>

<h4 id="the-layout">The layout</h4>

<p>This section is meant to show the data coming from the sensors of the robot (cameras, laser, odometry, etc.). For that purpose, the GUI is divided in sections conforming a <strong>layout</strong>. This disposition will come specified in the configuration file (see <em>Building your configuration file</em>) section.</p>

<figure class=" ">
  
    
      <a href="/assets/images/layout.png">
        <img src="/assets/images/layout.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>As you can see, there are several boxes or <strong>frames</strong> that will host data from different sensors. The view above shows the GUI before specifying what kind of sensor and data the frame will show. You only have to give the frame a <strong>name</strong> (if you leave it blank it will take the default name <em>frame_X</em> where X is the frame number), and pick which kind of data that frame will contain by clicking on one of the radio buttons.</p>

<figure class=" ">
  
    
      <a href="/assets/images/frame.png">
        <img src="/assets/images/frame.png" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>As you type down the name of the frame, you will see how the name in the top-left corner of the frame changes dynamically.</p>

<figure class=" ">
  
    
      <a href="/assets/images/rename.gif">
        <img src="/assets/images/rename.gif" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>

<p>Once you have chosen the frame name (this is important for later), you have to chose the data type the frame will show, from one of the checkboxes below the name textbox. After that, you will only have to click the <strong>Confirm</strong> button and the sensor will show its data.</p>

<figure class=" ">
  
    
      <a href="/assets/images/frame_config.gif">
        <img src="/assets/images/frame_config.gif" alt="" />
      </a>
    
  
  
    <figcaption>
</figcaption>
  
</figure>
:ET