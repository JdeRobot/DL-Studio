I" <h2 id="motivation">Motivation</h2>

<p>The infrastructure of this application is composed of different pieces of software, as shown in the image below. It is based on the Model-View-Controller (MVC) architecture where we have the <strong>model</strong> (the main logic of the application), the <strong>view</strong> (the user interface) and the <strong>controller</strong> that acts as a form of intercommunication of the previous ones.</p>

<figure class=" ">
  
    
      <a href="/assets/images/behavior_suite_diagram.png">
        <img src="/assets/images/behavior_suite_diagram.png" alt="" />
      </a>
    
  
  
    <figcaption>Deep Learning Studio Diagram
</figcaption>
  
</figure>

<p>The core of the application are the driver and the pilot modules, which are responsible for the management of the application and the management of the robot and behaviors respectively.</p>

<h2 id="progress">Progress</h2>

<h3 id="formula">Formula</h3>

<p>This part of the application is in a single module called <em>controller.py</em> and is responsible for the intercommunication between the model and the view. It is made in such a way that the model increases its performance without having to wait for the view to process the data.</p>

<h2 id="programming-a-custom-brain">Programming a custom brain</h2>

<p>Adding a new brain should first create a python file within the <code class="language-plaintext highlighter-rouge">brain</code> folder, this new brain should be a Brain Class module with the following basic functions that work with the GUI. When using a the <code class="language-plaintext highlighter-rouge">yml</code> file the robot type must match the name of the brain folder.</p>

<p><strong>__init__</strong></p>

<p>this function should have as variables the:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- sensors ( Sensors instance of the robot ).
- actuators ( Actuators instance of the robot ).
</code></pre></div></div>

<p><strong>update_frame</strong></p>

<p>Updates the information to be shown in one of the GUIâ€™s frames.</p>

<p><strong>update_pose</strong></p>

<p>Updates the pose 3D information obtained from the robot.</p>

<p><strong>execute</strong></p>

<p>Main loop of the brain. This will be called iteratively each TIME_CYCLE (see pilot.py)</p>

<h2 id="current-status">Current Status</h2>

<p>We are currently redesigning the project. The following <strong>functional requirements</strong> have been specified:</p>

<table>
  <thead>
    <tr>
      <th>Number</th>
      <th>Description</th>
      <th>Status</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RF01</td>
      <td>Changing run-time intelligence</td>
      <td>WIP</td>
    </tr>
    <tr>
      <td>RF02</td>
      <td>Save tagged dataset (IMG + ROSbags, cmd-vel)</td>
      <td>WIP</td>
    </tr>
    <tr>
      <td>RF03</td>
      <td>â€˜Manualâ€™ Autopilot. User solution (OpenCV)</td>
      <td>DONE</td>
    </tr>
    <tr>
      <td>RF04</td>
      <td>Teleoperation</td>
      <td>WIP</td>
    </tr>
    <tr>
      <td>RF05</td>
      <td>Benchmarking (neuronal network vs groundthruth, checkpoints, center desviation, â€¦)</td>
      <td>-</td>
    </tr>
    <tr>
      <td>RF06</td>
      <td>Support for different environments (TensorFlow, Keras, Pytorch, OpenCV, â€¦)</td>
      <td>WIP</td>
    </tr>
    <tr>
      <td>RF07</td>
      <td>User profiles (configuration file)</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p>The following table contains <strong>non-functional requirements</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Number</th>
      <th>Description</th>
      <th>Status</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RN01</td>
      <td>Real time</td>
      <td>-</td>
    </tr>
    <tr>
      <td>RN02</td>
      <td>Memory</td>
      <td>-</td>
    </tr>
    <tr>
      <td>RN03</td>
      <td>GPU-Ready</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<h2 id="references">References</h2>

<ul>
  <li>
    <p>Zhicheng Li, Zhihao Gu, Xuan Di, Rongye Shi. An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset.
<em>arXiv e-prints, art.arXiv:2002.05878</em>, Feb 2020. https://arxiv.org/abs/2002.05878</p>
  </li>
  <li>
    <p>Pei Sun et at. Scalability in Perception for Autonomous Driving: Waymo Open Dataset. 
<em>arXiv e-prints, art.arXiv:1912.04838</em>, Dec 2019. https://arxiv.org/abs/1912.04838 
(Waymo Open Dataset)[https://waymo.com/open/]</p>
  </li>
</ul>

:ET